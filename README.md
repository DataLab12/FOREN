# ViSIR: Vision Transformer Meets SIREN for Image Super-Resolution

**ViSIR** combines Vision Transformers (ViT) with Sinusoidal Representation Networks (SIRENs) to enhance image super-resolution tasks. The model introduces SIREN-based feedforward layers within the transformer architecture, enabling superior recovery of high-frequency details in the output image.

---

## ğŸ” Overview

ViSIR consists of the following components:

- **Patch Embedding**: Converts input images into non-overlapping patches for transformer processing.
- **Transformer Encoder**: Applies multi-head self-attention to model relationships between patches.
- **SIREN-Driven Feedforward**: Replaces the traditional MLP block with sinusoidal activation layers for better frequency learning.
- **SIREN Decoder**: Maps the processed representation to a high-resolution output image.

---

## ğŸ“ Directory Structure

â”œâ”€â”€ LR/  Folder with low-resolution input images
â”œâ”€â”€ HR/  Folder with high-resolution ground truth images
â”œâ”€â”€ FinalScoresVitSIRENF*.xlsx # Training results (loss, PSNR, SSIM, etc.)
â”œâ”€â”€ ViTSIREN2comp_image#*.png # Visualization comparisons of output vs ground truth
â”œâ”€â”€ visir_main.py # Main training script (contains model and pipeline)


## ğŸ§  Model Highlights

- Integrates a ViT encoder with a SIREN-based decoder.
- Uses positional embeddings for patch ordering.
- Enhances reconstruction quality using sine activation functions (SIREN).
- Outputs a high-resolution image that is 3Ã— larger than the input.

---

## ğŸ§ª Getting Started

### ğŸ”§ Requirements

Install dependencies with pip:

```bash
pip install torch torchvision matplotlib numpy pillow openpyxl scikit-image
```

ğŸ–¼ Dataset Preparation
Place your image pairs in the following folders:

LR/: Low-resolution images (e.g., 0.png)

HR/: High-resolution (3Ã—) images with the same filenames (e.g., 0.png)

â–¶ï¸ Run Training
Update the visir_main.py script as needed, then run:
